% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrartcl}
\usepackage{xcolor}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{5}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother





\setlength{\emergencystretch}{3em} % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



 


\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[section]
\theoremstyle{remark}
\AtBeginDocument{\renewcommand*{\proofname}{Proof}}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\newtheorem{refremark}{Remark}[section]
\newtheorem{refsolution}{Solution}[section]
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Bayesian Statistics Workbook},
  pdfauthor={Vu Nguyen Quang Duy},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Bayesian Statistics Workbook}
\author{Vu Nguyen Quang Duy}
\date{}
\begin{document}
\maketitle

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{3}
\tableofcontents
}

\section{Bayes' Rules}\label{bayes-rules}

\subsection{Chapter Summary}\label{chapter-summary}

\subsubsection{Conditional vs unconditional
probability}\label{conditional-vs-unconditional-probability}

Let \(A\) and \(B\) be two events, The \textbf{unconditional
probability} of \(A\), measures the probability of observing \(A\),
without any knowledge of \(B\). In contrast, the \textbf{conditional
probability} of \(A\) given \(B\), \(P(A|B)\), measures the probability
of observing \(A\) in light of the information that \(B\) occured.

Conditional probabilities are fundamental to Bayesian analyses. In
general, comparing the conditional vs unconditional probabilities,
\(P(A|B)\) vs \(P(A)\), reveals the extent to which information about
\(B\) informs our understanding of \(A\). In some cases, the certainty
of an event \(A\) might \emph{increase} or \emph{decrease} in light of
new data \(B\). In other words:

\[
P(A|B) > P(A) \text{ Or } P(A|B) < P(A)
\]

The \emph{order} of conditioning is also important. Since they measure
two it's typically the case that:

\[
P(A|B) \neq P(B|A)
\]

\subsubsection{Independent events}\label{independent-events}

Two events \(A\) and \(B\) are \textbf{independent} if and only if the
occurrence of \(B\) does not tell us anything about the occurrence of
\(A\):

\[
P(A|B) = P(A)
\]

\subsubsection{Probability vs
likelihood}\label{probability-vs-likelihood}

When \(B\) is known, the \textbf{conditional probability function}
\(P(\cdot|B)\) allows us to compare the probabilities of an unknown
event, \(A\) and \(\overline{A}\), occurring with \(B\):

\[
P(A|B) \text{ vs } P(\overline{A}|B)
\]

When \(A\) is known, the \textbf{likelyhood function}
\(L(\cdot|A) = P(A|\cdot)\) allows us to evaluate the relative
compatibility of data \(A\) with events \(B\) or \(\overline{B}\):

\[
L(B|A) \text{ vs } L(\overline{B}|A)
\]

\subsubsection{Joint and conditional
probabilities}\label{joint-and-conditional-probabilities}

For events \(A\) and \(B\), the joint probablity of \(A \cap B\) is
calculated by weighting the conditional probability of \(A\) given \(B\)
by the marginal probability of \(B\):

\begin{equation}\phantomsection\label{eq-21}{
P(A \cap B) = P(A|B)P(B)
}\end{equation}

Thus when \(A\) and \(B\) are \emph{independent},

\[
P(A\cap B) = P(A)P(B)
\]

Dividing both sides of Equation~\ref{eq-21} by \(P(B)\), and assuming
\(P(B) \neq 0\), reveals the definition of the conditional probability
of \(A\) given \(B\):

\begin{equation}\phantomsection\label{eq-22}{
P(A/B) = \frac{P(A \cap B)}{P(B)}
}\end{equation}

Thus, to evaluate the chance that \(A\) occurs in light of information
\(B\) we can consider the chance that they occur together,
\(P(A \cap B)\), relative to the chance that \(B\) occurs at all,
\(P(B)\)

\subsubsection{Law of Total Probability
(LTP)}\label{law-of-total-probability-ltp}

\begin{equation}\phantomsection\label{eq-23}{
P(A) = P(A \cap B) + P(A \cap \overline{B}) = P(A|B)P(B) + P(A|\overline{B})P(\overline{B})
}\end{equation}

\subsubsection{Bayes' Rule for events}\label{bayes-rule-for-events}

For events \(A\) and \(B\), the posterior probability of \(B\) given
\(A\) follows by combining Equation~\ref{eq-21} with
Equation~\ref{eq-22} and recognizing that we can evaluate data \(A\)
through the likelihood function, \(L(B|A) = P(A|B)\) and
\(L(\overline{B}|A) = P(A|\overline{B})\):

\begin{equation}\phantomsection\label{eq-24}{
P(B|A) = \frac{P(A \cap B)}{P(A)} = \frac{P(B)L(B|A)}{P(A)}
}\end{equation}

where by the Law of Total Probability Equation~\ref{eq-23}:

\begin{equation}\phantomsection\label{eq-25}{
P(A) = P(B)L(B|A) + P(\overline{B})L(\overline{B}|A)
}\end{equation}

More generally,

\[
\text{posterior} = \frac{\text{prior} \cdot \text{likelihood}}{\text{normalizing constant}}
\]

\subsubsection{Discrete probability
model}\label{discrete-probability-model}

Let \(Y\) be a discrete random variable. The probability model of \(Y\)
is specified by a \textbf{probability mass function (pmf) \(f(y)\)}. Thi
pmf defines the probability of any given outcome \(y\),

\[
f(y) = P(Y = y)
\]

and has the following properties:

\begin{itemize}
\item
  \(0 \leq f(y) \leq 1\) for all \(y\), and
\item
  \(\sum_{\text{all } y}f(y) = 1\), i.e., the probabilities of all
  possible outcomes of \(y\) sum to 1.
\end{itemize}

\subsubsection{\texorpdfstring{Conditional probability model of data
\(Y\)}{Conditional probability model of data Y}}\label{conditional-probability-model-of-data-y}

Let \(Y\) be a discrete random variable and \(\pi\) ve a parameter uop
which \(Y\) depends. The the conditional probability model of Y given
\(\pi\) is specified by conditional pmf \(f(y|\pi)\). This pmf specifies
the conditional probability of observing \(y\) given \(\pi\),

\[
f(y|\pi) = P(Y = y | \pi)
\]

and has the following properties:

\begin{itemize}
\item
  \(0 \leq f(y|\pi) \leq 1\) for all \(y\), and
\item
  \(\sum_{\text{all } y}f(y|\pi) = 1\)
\end{itemize}

\subsubsection{The Binomal model}\label{the-binomal-model}

Let random variable \(Y\) be the \emph{number of successes} in a
\emph{fixed number of trials} \(n\). Assume that the trials are
\emph{independent} and that the \emph{probability of success} in each
trial is \(pi\). Then the conditional depdendence of \(Y\) on \(\pi\)
can be modeled by the Binomal model with \textbf{parameters} \(n\) and
\(\pi\). In mathematical notation:

\[
Y|\pi \sim \text{Bin}(n, \pi)
\]

where ``\(\sim\)'' can be read as ``modeled by''. Correspondingly, the
Binomal model is specified by \textbf{conditional pmf}

\begin{equation}\phantomsection\label{eq-27}{
f(y|\pi) = \binom{n}{y} \pi ^2 \text{ for } y \in \{0,1,2, \dots ,n\}
}\end{equation}

\subsubsection{Probability mass functions vs likelihood
functions}\label{probability-mass-functions-vs-likelihood-functions}

When \(\pi\) is known, the \textbf{conditional pmf \(f(\cdot | \pi)\)}
allows us to compare the probabilites of different possible values of
data \(Y\) (e.g., \(y_1\) or \(y_2\)) occuring with \(\pi\):

\[
f(y_1|\pi) \text{ vs } f(y_2|\pi)
\]

When \(Y = y\) is known, the \textbf{likelihood function
\(L(\cdot|y) = f(y|\cdot)\)} allows us to compare the relative
likelihood of observing data \(y\) under different possible values of
\(\pi\) (e.g., \(\pi_1\) or \(\pi_2\)):

\[
L(\pi_1|y) \text{ vs } L(\pi_2|y)
\]

Thus, \(L(\cdot|y)\) provides the tool we need to evaluate the relative
compatibility of data \(Y = y\) with various \(\pi\) values.

\subsubsection{Bayes' Rule for
variables}\label{bayes-rule-for-variables}

For any variables \(\pi\) and \(Y\), let \(f(\pi)\) denote the prior pmf
of \(\pi\) and \(L(\pi|y)\) denote the likelihood function of \(\pi\)
given observed data \(Y=y\). Then the posterior pmf of \(\pi\) given
data \(Y = y\) is:

\begin{equation}\phantomsection\label{eq-2.12}{
f(\pi | y) = \frac{\text{prior} \cdot \text{likelihood}}{\text{normalizing constant}} = \frac{f(\pi) L(\pi|y)}{f(y)}
}\end{equation}

where, by the Law of Total Probability, the overall probability of
observing data \(Y = y\) accross all possible \(\pi\) is:

\[
f(y) = \sum_{\text{all } y} f(\pi) L(\pi|y)
\]

\subsubsection{Propotionality}\label{propotionality}

Since \(f(y)\) is merely a normalizing constant which does not depend of
\(\pi\), the posterior pmf \(f(\pi|y)\) is propotional to the product of
\(f(\pi)\) and \(L(\pi|y)\):

\[
f(\pi | y) = \frac{f(\pi) L(\pi|y)}{f(y)} \propto f(\pi)L(\pi|y)
\]

That is,

\[
\text{posterior} \propto \text{prior} \cdot \text{likelihood}
\]

The significance of this proportionality is that all the information we
need to build the posterior model is held in the prior and likelihood.

\pagebreak

\subsection{Excercises}\label{excercises}

\subsubsection{Buidling up to Bayes'
Rule}\label{buidling-up-to-bayes-rule}

\begin{exercise}[]\protect\hypertarget{exr-1_65}{}\label{exr-1_65}

\emph{Comparing the prior and posterior}

For each scenario below, you're given a pair of events, \(A\) and \(B\).
Explain what you believe to be the relationship between the posterior
and prior probabilities of \(B\): \(P(B|A) > P(B)\) or \(P(B|A) < P(B)\)

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\item
  \(A=\) you just finished reading Lambda Literary Award-winning author
  Nicole Dennis-Benn's first novel, and you enjoyed it! \(B=\) you will
  also enjot Benn's newest novel.
\item
  \(A=\) it's 0 degrees Fahrenheit in Minnesota on a January day. \(B=\)
  it will be 60 degrees tomorrow.
\item
  \(A=\) the authors only got 3 hours of sleep last night. \(B=\) the
  authors make several typos in their writing today.
\item
  \(A=\) your friend includes three hashtags in their tweet. \(B=\) the
  tweet gets retweeted.
\end{enumerate}

\end{exercise}

\textbf{Solution}

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\tightlist
\item
  \textbf{Answer}: \(P(B|A) > P(B)\)
\end{enumerate}

\begin{itemize}
\item
  The prior probability, \(P(B)\): The general probability of enjoying
  Benn's newest novel before reading any of her previous work.
\item
  The posterior probability, \(P(B∣A)\): The updated probability of
  enjoying Benn's newest novel, given that her first novel was read and
  enjoyed.
\end{itemize}

The event \(A\) (enjoying the first novel) is positive evidence that
provides a reason to increase belief in event \(B\) (enjoying the newest
novel). A favorable experience with the author's work makes the updated
belief (the posterior) stronger and therefore higher than the initial
belief (the prior).

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{Answer}: \(P(B|A) < P(B)\)
\end{enumerate}

\begin{itemize}
\item
  The prior probability, \(P(B)\): The general probability that it will
  be 60 degrees tomorrow.
\item
  The posterior probability, \(P(B∣A)\): The updated probability that it
  will be 60 degrees tomorrow, given that it was 0 degrees Fahrenheit
  yesterday.
\end{itemize}

The event \(A\) (a temperature of 0°F yesterday) is negative evidence
that provides a reason to decrease the belief in event \(B\) (a
temperature of 60°F tomorrow). A temperature of 0°F makes it
significantly less likely that the temperature will be a relatively mild
60°F the next day. This new information acts as negative evidence,
causing a decrease in the belief of event B.

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{Answer}: \(P(B|A) > P(B)\)
\end{enumerate}

\begin{itemize}
\item
  The prior probability, \(P(B)\): The general probability that the
  authors will make several typos in their writing today.
\item
  The posterior probability, \(P(B∣A)\): The updated probability that
  the authors will make several typos, given they only got 3 hours of
  sleep last night.
\end{itemize}

The event \(A\) (only 3 hours of sleep) is positive evidence that
increases the probability of event \(B\) (making typos). Lack of sleep
is a well-known factor that impairs cognitive function and attention to
detail, making errors like typos more probable. The updated belief is
therefore higher than the initial belief.

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\setcounter{enumi}{3}
\tightlist
\item
  \textbf{Answer}: \(P(B|A) > P(B)\)
\end{enumerate}

\begin{itemize}
\item
  The prior probability, \(P(B)\): The general probability that the
  tweet will be retweeted. This is the baseline likelihood without
  knowing anything about the tweet's content or format.
\item
  The posterior probability, \(P(B∣A)\): he updated probability that the
  tweet will be retweeted, given that it includes three hashtags.
\end{itemize}

The event \(A\) (including three hashtags) is positive evidence that
increases the probability of event \(B\) (the tweet being retweeted).
Research on social media engagement shows that tweets with hashtags,
especially a moderate number, tend to have wider reach and higher
engagement, which includes retweets. Therefore, the updated belief is
higher than the initial belief.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{exercise}[]\protect\hypertarget{exr-2_66}{}\label{exr-2_66}

\emph{Marginal, conditional, or joint?}

Define the following events for a resident of a fictional town:

\begin{itemize}
\item
  \(A=\) drives 10 miles per hour above the speed limit,
\item
  \(B=\) gets a speeding ticket,
\item
  \(C=\) took statistics at the local college,
\item
  \(D=\) has used R,
\item
  \(E=\) likes the music of Prince,
\item
  \(F=\) is a Minnesotan.
\end{itemize}

Several facts about these events are listed below. Specify each of these
facts using probability notation, paying special attention to whether
it's a marginal, conditional, or joint probability.

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\item
  73\% of people that drive 10 miles per hour above the speed limit get
  a speeding ticket.
\item
  20\% of residents drive 10 miles per hour above the speed limit.
\item
  15\% of residents have used R.
\item
  91\% of statistics students at the local college have used R.
\item
  38\% of residents are Minnesotans that like the music of Prince.
\item
  95\% of the Minnesotan residents like the music of Prince.
\end{enumerate}

\end{exercise}

\textbf{Solution}

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\tightlist
\item
  \textbf{Answer}: conditional probability
\end{enumerate}

This fact gives the probability of getting a speeding ticket (\(B\))
given that a person is already driving 10 miles per hour above the speed
limit (\(A\)).

The probability notation for this is: \(P(B|A) = 0.73\)

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{Answer}: marginal probability
\end{enumerate}

This facts only describe the proportion of residents that drives 10
miles per hour above the speed limit (\(A\)) without any conditions.

The probability notation for this is: \(P(A) = 0.20\)

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{Answer}: marginal probability
\end{enumerate}

This facts only describe the proportion of residents that have used R
(\(D\)) without any conditions.

The probability notation for this is: \(P(D) = 0.15\)

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\setcounter{enumi}{3}
\tightlist
\item
  \textbf{Answer}: conditional probability
\end{enumerate}

This fact It states the probability of a person having used R (\(D\))
given that they took statistics at the local college (\(C\)).

The probability notation for this is: \(P(D|C) = 0.91\)

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\setcounter{enumi}{4}
\tightlist
\item
  \textbf{Answer}: joint probability
\end{enumerate}

This is a joint probability because it refers to the likelihood of two
events happening at the same time: being a Minnesotan and liking the
music of Prince.

The probability notation for this is: \(P(E \cap F) = 0.38\)

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\setcounter{enumi}{5}
\tightlist
\item
  \textbf{Answer}: conditional probability
\end{enumerate}

This is a conditional probability. It states the probability of a person
liking the music of Prince (\(E\)) given that they are a Minnesotan
(\(F\)).

The probability notation for this is: \(P(E|F) = 0.95\)




\end{document}
